{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "TTS_PATH = \"TTS/\"\n",
    "\n",
    "# add libraries into environment\n",
    "sys.path.append(TTS_PATH) # set this if TTS is not installed globally\n",
    "\n",
    "import TTS\n",
    "import os\n",
    "import string\n",
    "import time\n",
    "import argparse\n",
    "import json\n",
    "\n",
    "import numpy as np\n",
    "import IPython\n",
    "from IPython.display import Audio\n",
    "\n",
    "\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Skipping pytorch as it is not installed.\u001b[0m\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'TTS.utils'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-0221dee428be>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mTTS\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownloaders\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdownload_ljspeech\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mdownload_ljspeech\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"../ljspeech\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'TTS.utils'"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Template code from coqui ai documentation - Building a model\n",
    "\n",
    "from TTS.tts.models.base_tts import BaseTTS\n",
    "\n",
    "\n",
    "class MyModel(BaseTTS):\n",
    "    \"\"\"\n",
    "    Notes on input/output tensor shapes:\n",
    "        Any input or output tensor of the model must be shaped as\n",
    "\n",
    "        - 3D tensors `batch x time x channels`\n",
    "        - 2D tensors `batch x channels`\n",
    "        - 1D tensors `batch x 1`\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config: Coqpit):\n",
    "        super().__init__()\n",
    "        self._set_model_args(config)\n",
    "\n",
    "    def _set_model_args(self, config: Coqpit):\n",
    "        \"\"\"Set model arguments from the config. Override this.\"\"\"\n",
    "        pass\n",
    "\n",
    "    def forward(self, input: torch.Tensor, *args, aux_input={}, **kwargs) -> Dict:\n",
    "        \"\"\"Forward pass for the model mainly used in training.\n",
    "\n",
    "        You can be flexible here and use different number of arguments and argument names since it is intended to be\n",
    "        used by `train_step()` without exposing it out of the model.\n",
    "\n",
    "        Args:\n",
    "            input (torch.Tensor): Input tensor.\n",
    "            aux_input (Dict): Auxiliary model inputs like embeddings, durations or any other sorts of inputs.\n",
    "\n",
    "        Returns:\n",
    "            Dict: Model outputs. Main model output must be named as \"model_outputs\".\n",
    "        \"\"\"\n",
    "        outputs_dict = {\"model_outputs\": None}\n",
    "        ...\n",
    "        return outputs_dict\n",
    "\n",
    "    def inference(self, input: torch.Tensor, aux_input={}) -> Dict:\n",
    "        \"\"\"Forward pass for inference.\n",
    "\n",
    "        We don't use `*kwargs` since it is problematic with the TorchScript API.\n",
    "\n",
    "        Args:\n",
    "            input (torch.Tensor): [description]\n",
    "            aux_input (Dict): Auxiliary inputs like speaker embeddings, durations etc.\n",
    "\n",
    "        Returns:\n",
    "            Dict: [description]\n",
    "        \"\"\"\n",
    "        outputs_dict = {\"model_outputs\": None}\n",
    "        ...\n",
    "        return outputs_dict\n",
    "\n",
    "    def train_step(self, batch: Dict, criterion: nn.Module) -> Tuple[Dict, Dict]:\n",
    "        \"\"\"Perform a single training step. Run the model forward pass and compute losses.\n",
    "\n",
    "        Args:\n",
    "            batch (Dict): Input tensors.\n",
    "            criterion (nn.Module): Loss layer designed for the model.\n",
    "\n",
    "        Returns:\n",
    "            Tuple[Dict, Dict]: Model ouputs and computed losses.\n",
    "        \"\"\"\n",
    "        outputs_dict = {}\n",
    "        loss_dict = {}  # this returns from the criterion\n",
    "        ...\n",
    "        return outputs_dict, loss_dict\n",
    "\n",
    "    def train_log(self, batch: Dict, outputs: Dict, logger: \"Logger\", assets:Dict, steps:int) -> None:\n",
    "        \"\"\"Create visualizations and waveform examples for training.\n",
    "\n",
    "        For example, here you can plot spectrograms and generate sample sample waveforms from these spectrograms to\n",
    "        be projected onto Tensorboard.\n",
    "\n",
    "        Args:\n",
    "            ap (AudioProcessor): audio processor used at training.\n",
    "            batch (Dict): Model inputs used at the previous training step.\n",
    "            outputs (Dict): Model outputs generated at the previoud training step.\n",
    "\n",
    "        Returns:\n",
    "            Tuple[Dict, np.ndarray]: training plots and output waveform.\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    def eval_step(self, batch: Dict, criterion: nn.Module) -> Tuple[Dict, Dict]:\n",
    "        \"\"\"Perform a single evaluation step. Run the model forward pass and compute losses. In most cases, you can\n",
    "        call `train_step()` with no changes.\n",
    "\n",
    "        Args:\n",
    "            batch (Dict): Input tensors.\n",
    "            criterion (nn.Module): Loss layer designed for the model.\n",
    "\n",
    "        Returns:\n",
    "            Tuple[Dict, Dict]: Model ouputs and computed losses.\n",
    "        \"\"\"\n",
    "        outputs_dict = {}\n",
    "        loss_dict = {}  # this returns from the criterion\n",
    "        ...\n",
    "        return outputs_dict, loss_dict\n",
    "\n",
    "    def eval_log(self, batch: Dict, outputs: Dict, logger: \"Logger\", assets:Dict, steps:int) -> None:\n",
    "        \"\"\"The same as `train_log()`\"\"\"\n",
    "        pass\n",
    "\n",
    "    def load_checkpoint(self, config: Coqpit, checkpoint_path: str, eval: bool = False) -> None:\n",
    "        \"\"\"Load a checkpoint and get ready for training or inference.\n",
    "\n",
    "        Args:\n",
    "            config (Coqpit): Model configuration.\n",
    "            checkpoint_path (str): Path to the model checkpoint file.\n",
    "            eval (bool, optional): If true, init model for inference else for training. Defaults to False.\n",
    "        \"\"\"\n",
    "        ...\n",
    "\n",
    "    def get_optimizer(self) -> Union[\"Optimizer\", List[\"Optimizer\"]]:\n",
    "        \"\"\"Setup an return optimizer or optimizers.\"\"\"\n",
    "        pass\n",
    "\n",
    "    def get_lr(self) -> Union[float, List[float]]:\n",
    "        \"\"\"Return learning rate(s).\n",
    "\n",
    "        Returns:\n",
    "            Union[float, List[float]]: Model's initial learning rates.\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    def get_scheduler(self, optimizer: torch.optim.Optimizer):\n",
    "        pass\n",
    "\n",
    "    def get_criterion(self):\n",
    "        pass\n",
    "\n",
    "    def format_batch(self):\n",
    "        pass\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example from coqui AI documentation - Training glow tts model\n",
    "\n",
    "\n",
    "import os\n",
    "\n",
    "# Trainer: Where the ‚ú®Ô∏è happens.\n",
    "# TrainingArgs: Defines the set of arguments of the Trainer.\n",
    "from trainer import Trainer, TrainerArgs\n",
    "\n",
    "# GlowTTSConfig: all model related values for training, validating and testing.\n",
    "from TTS.tts.configs.glow_tts_config import GlowTTSConfig\n",
    "\n",
    "# BaseDatasetConfig: defines name, formatter and path of the dataset.\n",
    "from TTS.tts.configs.shared_configs import BaseDatasetConfig\n",
    "from TTS.tts.datasets import load_tts_samples\n",
    "from TTS.tts.models.glow_tts import GlowTTS\n",
    "from TTS.tts.utils.text.tokenizer import TTSTokenizer\n",
    "from TTS.utils.audio import AudioProcessor\n",
    "\n",
    "# we use the same path as this script as our training folder.\n",
    "output_path = os.path.dirname(os.path.abspath(__file__))\n",
    "\n",
    "# DEFINE DATASET CONFIG\n",
    "# Set LJSpeech as our target dataset and define its path.\n",
    "# You can also use a simple Dict to define the dataset and pass it to your custom formatter.\n",
    "dataset_config = BaseDatasetConfig(\n",
    "    name=\"ljspeech\", meta_file_train=\"metadata.csv\", path=os.path.join(output_path, \"../LJSpeech-1.1/\")\n",
    ")\n",
    "\n",
    "# INITIALIZE THE TRAINING CONFIGURATION\n",
    "# Configure the model. Every config class inherits the BaseTTSConfig.\n",
    "config = GlowTTSConfig(\n",
    "    batch_size=32,\n",
    "    eval_batch_size=16,\n",
    "    num_loader_workers=4,\n",
    "    num_eval_loader_workers=4,\n",
    "    run_eval=True,\n",
    "    test_delay_epochs=-1,\n",
    "    epochs=1000,\n",
    "    text_cleaner=\"phoneme_cleaners\",\n",
    "    use_phonemes=True,\n",
    "    phoneme_language=\"en-us\",\n",
    "    phoneme_cache_path=os.path.join(output_path, \"phoneme_cache\"),\n",
    "    print_step=25,\n",
    "    print_eval=False,\n",
    "    mixed_precision=True,\n",
    "    output_path=output_path,\n",
    "    datasets=[dataset_config],\n",
    ")\n",
    "\n",
    "# INITIALIZE THE AUDIO PROCESSOR\n",
    "# Audio processor is used for feature extraction and audio I/O.\n",
    "# It mainly serves to the dataloader and the training loggers.\n",
    "ap = AudioProcessor.init_from_config(config)\n",
    "\n",
    "# INITIALIZE THE TOKENIZER\n",
    "# Tokenizer is used to convert text to sequences of token IDs.\n",
    "# If characters are not defined in the config, default characters are passed to the config\n",
    "tokenizer, config = TTSTokenizer.init_from_config(config)\n",
    "\n",
    "# LOAD DATA SAMPLES\n",
    "# Each sample is a list of ```[text, audio_file_path, speaker_name]```\n",
    "# You can define your custom sample loader returning the list of samples.\n",
    "# Or define your custom formatter and pass it to the `load_tts_samples`.\n",
    "# Check `TTS.tts.datasets.load_tts_samples` for more details.\n",
    "train_samples, eval_samples = load_tts_samples(dataset_config, eval_split=True)\n",
    "\n",
    "# INITIALIZE THE MODEL\n",
    "# Models take a config object and a speaker manager as input\n",
    "# Config defines the details of the model like the number of layers, the size of the embedding, etc.\n",
    "# Speaker manager is used by multi-speaker models.\n",
    "model = GlowTTS(config, ap, tokenizer, speaker_manager=None)\n",
    "\n",
    "# INITIALIZE THE TRAINER\n",
    "# Trainer provides a generic API to train all the üê∏TTS models with all its perks like mixed-precision training,\n",
    "# distributed training, etc.\n",
    "trainer = Trainer(\n",
    "    TrainerArgs(), config, output_path, model=model, train_samples=train_samples, eval_samples=eval_samples\n",
    ")\n",
    "\n",
    "# AND... 3,2,1... üöÄ\n",
    "trainer.fit()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TTS",
   "language": "python",
   "name": "corquitts"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
